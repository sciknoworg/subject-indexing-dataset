{
    "@graph": [
        {
            "@id": "gnd:4226127-2",
            "sameAs": "Neuronales Netz"
        },
        {
            "@id": "gnd:4825546-4",
            "sameAs": "Best\u00e4rkendes Lernen (K\u00fcnstliche Intelligenz)"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1020689897",
            "@type": "bibo:Book",
            "P1053": "1 Online-Ressource (327 Seiten)",
            "creator": "Dutta, Sayon",
            "description": "Erworben aus Studienqualit\u00e4tsmitteln",
            "identifier": [
                "(firstid)GBV:1020689897",
                "(ppn)1020689897",
                "(isbn13)9781788830713"
            ],
            "publisher": "Packt Publishing Ltd",
            "subject": [
                "Reinforcement learning..",
                "Artificial intelligence",
                "(classificationName=bk, id=10641240X)54.72 - K\u00fcnstliche Intelligenz",
                "Reinforcement learning",
                "(classificationName=ddc)006.31",
                "Neural networks (Computer science)",
                "Electronic books",
                "(classificationName=linsearch:mapping)inf",
                "Reinforcement learning."
            ],
            "title": "Reinforcement Learning with TensorFlow : A Beginner's Guide to Designing Self-Learning Systems with TensorFlow and OpenAI Gym",
            "abstract": [
                "Chapter 3: Markov Decision Process -- Markov decision processes -- The Markov property -- The S state set -- Actions -- Transition model -- Rewards -- Policy -- The sequence of rewards - assumptions -- The infinite horizons -- Utility of sequences -- The Bellman equations -- Solving the Bellman equation to find policies -- An example of value iteration using the Bellman equation -- Policy iteration -- Partially observable Markov decision processes -- State estimation -- Value iteration in POMDPs -- Training the FrozenLake-v0 environment using MDP -- Summary -- Chapter 4: Policy Gradients -- The policy optimization method -- Why policy optimization methods? -- Why stochastic policy? -- Example 1 - rock, paper, scissors -- Example 2 - state aliased grid-world -- Policy objective functions -- Policy Gradient Theorem -- Temporal difference rule -- TD(1) rule -- TD(0) rule -- TD() rule -- Policy gradients -- The Monte Carlo policy gradient -- Actor-critic algorithms -- Using a baseline to reduce variance -- Vanilla policy gradient -- Agent learning pong using policy gradients -- Summary -- Chapter 5: Q-Learning and Deep Q-Networks -- Why reinforcement learning? -- Model based learning and model free learning -- Monte Carlo learning -- Temporal difference learning -- On-policy and off-policy learning -- Q-learning -- The exploration exploitation dilemma -- Q-learning for the mountain car problem in OpenAI gym -- Deep Q-networks -- Using a convolution neural network instead of a single layer neural network -- Use of experience replay -- Separate target network to compute the target Q-values -- Advancements in deep Q-networks and beyond -- Double DQN -- Dueling DQN -- Deep Q-network for mountain car problem in OpenAI gym -- Deep Q-network for Cartpole problem in OpenAI gym -- Deep Q-network for Atari Breakout in OpenAI gym",
                "Reinforcement learning in robotics -- Evolution of reinforcement learning -- Challenges in robot reinforcement learning -- High dimensionality problem -- Real-world challenges -- Issues due to model uncertainty -- What's the final objective a robot wants to achieve? -- Open questions and practical challenges -- Open questions -- Practical challenges for robotic reinforcement learning -- Key takeaways -- Summary -- Chapter 12: Deep Reinforcement Learning in Ad Tech -- Computational advertising challenges and bidding strategies -- Business models used in advertising -- Sponsored-search advertisements -- Search-advertisement management -- Adwords -- Bidding strategies of advertisers -- Real-time bidding by reinforcement learning in display advertising -- Summary -- Chapter 13: Reinforcement Learning in Image Processing -- Hierarchical object detection with deep reinforcement learning -- Related works -- Region-based convolution neural networks -- Spatial pyramid pooling networks -- Fast R-CNN -- Faster R-CNN -- You Look Only Once -- Single Shot Detector -- Hierarchical object detection model -- State -- Actions -- Reward -- Model and training -- Training specifics -- Summary -- Chapter 14: Deep Reinforcement Learning in NLP -- Text summarization -- Deep reinforced model for Abstractive Summarization -- Neural intra-attention model -- Intra-temporal attention on input sequence while decoding -- Intra-decoder attention -- Token generation and pointer -- Hybrid learning objective -- Supervised learning with teacher forcing -- Policy learning -- Mixed training objective function -- Text question answering -- Mixed objective and deep residual coattention for Question Answering -- Deep residual coattention encoder -- Mixed objective using self-critical policy learning -- Summary -- Appendix : Further topics in Reinforcement Learning",
                "Cover -- Title Page -- Copyright and Credits -- Packt Upsell -- Contributors -- Table of Contents -- Preface -- Chapter 1: Deep Learning - Architectures and Frameworks -- Deep learning -- Activation functions for deep learning -- The sigmoid function -- The tanh function -- The softmax function -- The rectified linear unit function -- How to choose the right activation function -- Logistic regression as a neural network -- Notation -- Objective -- The cost function -- The gradient descent algorithm -- The computational graph -- Steps to solve logistic regression using gradient descent -- What is xavier initialization? -- Why do we use xavier initialization? -- The neural network model -- Recurrent neural networks -- Long Short Term Memory Networks -- Convolutional neural networks -- The LeNet-5 convolutional neural network -- The AlexNet model -- The VGG-Net model -- The Inception model -- Limitations of deep learning -- The vanishing gradient problem -- The exploding gradient problem -- Overcoming the limitations of deep learning -- Reinforcement learning -- Basic terminologies and conventions -- Optimality criteria -- The value function for optimality -- The policy model for optimality -- The Q-learning approach to reinforcement learning -- Asynchronous advantage actor-critic -- Introduction to TensorFlow and OpenAI Gym -- Basic computations in TensorFlow -- An introduction to OpenAI Gym -- The pioneers and breakthroughs in reinforcement learning -- David Silver -- Pieter Abbeel -- Google DeepMind -- The AlphaGo program -- Libratus -- Summary -- Chapter 2: Training Reinforcement Learning Agents Using OpenAI Gym -- The OpenAI Gym -- Understanding an OpenAI Gym environment -- Programming an agent using an OpenAI Gym environment -- Q-Learning -- The Epsilon-Greedy approach -- Using the Q-Network for real-world applications -- Summary",
                "Reinforcement learning allows you to develop intelligent, self-learning systems. This book shows you how to put the concepts of Reinforcement Learning to train efficient models.You will use popular reinforcement learning algorithms to implement use-cases in image processing and NLP, by combining the power of TensorFlow and OpenAI Gym.",
                "Continuous action space algorithms -- Trust region policy optimization -- Deterministic policy gradients -- Scoring mechanism in sequential models in NLP -- BLEU -- What is BLEU score and what does it do? -- ROUGE -- Summary -- Other Books You May Enjoy -- Index",
                "The Monte Carlo tree search algorithm -- Minimax and game trees -- The Monte Carlo Tree Search -- The SARSA algorithm -- SARSA algorithm for mountain car problem in OpenAI gym -- Summary -- Chapter 6: Asynchronous Methods -- Why asynchronous methods? -- Asynchronous one-step Q-learning -- Asynchronous one-step SARSA -- Asynchronous n-step Q-learning -- Asynchronous advantage actor critic -- A3C for Pong-v0 in OpenAI gym -- Summary -- Chapter 7: Robo Everything - Real Strategy Gaming -- Real-time strategy games -- Reinforcement learning and other approaches -- Online case-based planning -- Drawbacks to real-time strategy games -- Why reinforcement learning? -- Reinforcement learning in RTS gaming -- Deep autoencoder -- How is reinforcement learning better? -- Summary -- Chapter 8: AlphaGo - Reinforcement Learning at Its Best -- What is Go? -- Go versus chess -- How did DeepBlue defeat Gary Kasparov? -- Why is the game tree approach no good for Go? -- AlphaGo - mastering Go -- Monte Carlo Tree Search -- Architecture and properties of AlphaGo -- Energy consumption analysis - Lee Sedol versus AlphaGo -- AlphaGo Zero -- Architecture and properties of AlphaGo Zero -- Training process in AlphaGo Zero -- Summary -- Chapter 9: Reinforcement Learning in Autonomous Driving -- Machine learning for autonomous driving -- Reinforcement learning for autonomous driving -- Creating autonomous driving agents -- Why reinforcement learning ? -- Proposed frameworks for autonomous driving -- Spatial aggregation -- Sensor fusion -- Spatial features -- Recurrent temporal aggregation -- Planning -- DeepTraffic - MIT simulator for autonomous driving -- Summary -- Chapter 10: Financial Portfolio Management -- Introduction -- Problem definition -- Data preparation -- Reinforcement learning -- Further improvements -- Summary -- Chapter 11: Reinforcement Learning in Robotics"
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "isPartOf": [
                "(collectioncode)BSZ-30-PQE-K1DLR",
                "(collectioncode)ZDB-30-PQE",
                "(collectioncode)ZDB-30-PAD"
            ],
            "issued": "2018",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "commercial licence",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:4226127-2"
                },
                {
                    "@id": "gnd:4825546-4"
                }
            ],
            "P60163": "Birmingham"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "contributor": "http://purl.org/dc/terms/contributor",
        "issued": "http://purl.org/dc/terms/issued",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "description": "http://purl.org/dc/elements/1.1/description",
        "license": "http://purl.org/dc/terms/license",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "abstract": "http://purl.org/dc/terms/abstract",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "title": "http://purl.org/dc/elements/1.1/title",
        "creator": "http://purl.org/dc/elements/1.1/creator",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}