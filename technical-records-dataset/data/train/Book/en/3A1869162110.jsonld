{
    "@graph": [
        {
            "@id": "gnd:118271059X",
            "sameAs": "Bellemare, Marc G."
        },
        {
            "@id": "gnd:130803377X",
            "sameAs": "Dabney, Will"
        },
        {
            "@id": "gnd:130803380X",
            "sameAs": "Rowland, Mark"
        },
        {
            "@id": "gnd:4152404-4",
            "sameAs": "Entscheidungsfunktion"
        },
        {
            "@id": "gnd:4455835-1",
            "sameAs": "Agent (Informatik)"
        },
        {
            "@id": "gnd:4825546-4",
            "sameAs": "Best\u00e4rkendes Lernen (K\u00fcnstliche Intelligenz)"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1869162110",
            "@type": "bibo:Book",
            "P1053": "1 Online-Ressource (384 p.)",
            "identifier": [
                "(firstid)KEP:096795581",
                "(isbn13)9780262048019",
                "(ppn)1869162110",
                "(isbn13)9780262374026"
            ],
            "publisher": "The MIT Press",
            "subject": [
                "Reinforcement learning",
                "(classificationName=ddc)519.6",
                "Probability & statistics",
                "Machine learning",
                "(classificationName=bk, id=10641240X)54.72 - K\u00fcnstliche Intelligenz",
                "(classificationName=ddc)006.3/1",
                "(classificationName=linsearch:mapping)inf",
                "(classificationName=loc)Q325.6"
            ],
            "title": "Distributional Reinforcement Learning",
            "abstract": [
                "The first comprehensive guide to distributional reinforcement learning, providing a new mathematical formalism for thinking about decisions from a probabilistic perspective.Distributional reinforcement learning is a new mathematical formalism for thinking about decisions. Going beyond the common approach to reinforcement learning and expected values, it focuses on the total reward or return obtained as a consequence of an agent's choices\u2014specifically, how this return behaves from a probabilistic perspective. In this first comprehensive guide to distributional reinforcement learning, Marc G. Bellemare, Will Dabney, and Mark Rowland, who spearheaded development of the field, present its key concepts and review some of its many applications. They demonstrate its power to account for many complex, interesting phenomena that arise from interactions with one's environment.The authors present core ideas from classical reinforcement learning to contextualize distributional topics and include mathematical proofs pertaining to major results discussed in the text. They guide the reader through a series of algorithmic and mathematical developments that, in turn, characterize, compute, estimate, and make decisions on the basis of the random return. Practitioners in disciplines as diverse as finance (risk management), computational neuroscience, computational psychiatry, psychology, macroeconomics, and robotics are already using distributional reinforcement learning, paving the way for its expanding applications in mathematical finance, engineering, and the life sciences. More than a mathematical approach, distributional reinforcement learning represents a new perspective on how intelligent agents make predictions and decisions",
                "\"Distributional reinforcement learning provides a mathematical theory to describe the random outcomes caused by an agent's decisions\"--"
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": [
                "gnd:118271059X",
                "gnd:130803377X",
                "gnd:130803380X"
            ],
            "isPartOf": "(collectioncode)ZDB-94-OAB",
            "issued": "2023",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:4825546-4"
                },
                {
                    "@id": "gnd:4455835-1"
                },
                {
                    "@id": "gnd:4152404-4"
                }
            ],
            "P30128": "Adaptive Computation and Machine Learning series",
            "P60163": "Cambridge"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "license": "http://purl.org/dc/terms/license",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "title": "http://purl.org/dc/elements/1.1/title",
        "issued": "http://purl.org/dc/terms/issued",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "abstract": "http://purl.org/dc/terms/abstract",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "P30128": "http://www.rdaregistry.info/Elements/m/#P30128",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "contributor": "http://purl.org/dc/terms/contributor",
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}